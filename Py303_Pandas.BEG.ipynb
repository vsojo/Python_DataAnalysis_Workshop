{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to `pandas` üêº\n",
    "(V√≠ctor Sojo | vsojo@amnh.org)\n",
    "\n",
    "In this notebook we will learn how to use `pandas`, a mighty data-analysis tool for Python. The main object in `pandas` is the `DataFrame`, which is a type of dynamic table (similar to an Excel spreadsheet) in which you can access, filter, sort and manipulate columns and rows in extremely efficient and powerful ways.\n",
    "\n",
    "These `pandas.DataFrame`s are somewhat similar to an MS Excel spreadsheet, but far more powerful. If you have ever worked in R, `pandas`' `DataFrame`s work very similarly to the ones in R (in fact, in the following lesson we will learn that we can easily convert between one and the other within Jupyter).\n",
    "\n",
    "\n",
    "**References:**\n",
    "+ I downloaded (and heavily rearranged) quite a bit of data from the [World Bank's Open Data archive](https://data.worldbank.org/indicator).\n",
    "+ The official [\"10-minute\" guide to pandas](https://pandas.pydata.org/docs/user_guide/10min.html) is meant to be geared towards beginners, but it makes quite a few assumptions on previous knowledge of both Python and NumPy.\n",
    "+ Pandas also have an official introductory video [here](https://pandas.pydata.org/getting_started.html), but many viewers may also find it rather advanced to be introductory.\n",
    "+ Instead ‚Äî and other than the lesson below ‚Äî [this tutorial on YouTube](https://www.youtube.com/watch?v=vmEHCJofslg) by Keith Galli is a great introduction.\n",
    "+ Once you have a good basis of `pandas`, I strongly recommend their official documentation, including their [list of specific beginner tutorials](https://pandas.pydata.org/docs/getting_started/index.html#intro-to-pandas), as well as their very thorough and helpful documentation for specific methods and tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "&emsp;[Getting started](#Getting-started)<br/>\n",
    "&emsp;[Creating a pandas.DataFrame](#Creating-a-pandas.DataFrame)<br/>\n",
    "&emsp;[Exploring the size\\(s\\) of a dataframe](#Exploring-the-size\\(s\\)-of-a-dataframe)<br/>\n",
    "&emsp;[Accessing columns in a dataframe](#Accessing-columns-in-a-dataframe)<br/>\n",
    "&emsp;[Arithmetic operations across entire columns](#Arithmetic-operations-across-entire-columns)<br/>\n",
    "&emsp;[Adding columns to a dataframe](#Adding-columns-to-a-dataframe)<br/>\n",
    "&emsp;[Deleting or \\\"dropping\\\" columns](#Deleting-or-\\\"dropping\\\"-columns)<br/>\n",
    "&emsp;[Renaming columns](#Renaming-columns)<br/>\n",
    "&emsp;[Applying functions on columns](#Applying-functions-on-columns)<br/>\n",
    "&emsp;[Getting the data types of columns](#Getting-the-data-types-of-columns)<br/>\n",
    "&emsp;[Boolean operations and filtering/subsetting the rows in a dataframe](#Boolean-operations-and-filtering/subsetting-the-rows-in-a-dataframe)<br/>\n",
    "&emsp;[Using .loc\\(\\) and .iloc\\(\\) to filter and explore rows and columns](#Using-.loc\\(\\)-and-.iloc\\(\\)-to-filter-and-explore-rows-and-columns)<br/>\n",
    "&emsp;[Using .loc \\(or .iloc\\) to replace values in a dataframe](#Using-.loc-\\(or-.iloc\\)-to-replace-values-in-a-dataframe)<br/>\n",
    "&emsp;[Writing and reading data to/from text files](#Writing-and-reading-data-to/from-text-files)<br/>\n",
    "&emsp;[Loading in a more realistic dataset](#Loading-in-a-more-realistic-dataset)<br/>\n",
    "&emsp;[Use pivot to transform a stacked dataframe to a multi-indexed one](#Use-pivot-to-transform-a-stacked-dataframe-to-a-multi-indexed-one)<br/>\n",
    "&emsp;[Flattening a multi-index dataframe](#Flattening-a-multi-index-dataframe)<br/>\n",
    "&emsp;[Resetting the index of a dataframe](#Resetting-the-index-of-a-dataframe)<br/>\n",
    "&emsp;[Extracting more than a sample of rows](#Extracting-more-than-a-sample-of-rows)<br/>\n",
    "&emsp;[Summary statistics](#Summary-statistics)<br/>\n",
    "&emsp;[Merging dataframes](#Merging-dataframes)<br/>\n",
    "&emsp;[Comparing two columns](#Comparing-two-columns)<br/>\n",
    "&emsp;[Replacing specific values in a dataframe with .replace\\(\\)](#Replacing-specific-values-in-a-dataframe-with-.replace\\(\\))<br/>\n",
    "&emsp;[Computing aggregate statistics per group with .groupby\\(\\)](#Computing-aggregate-statistics-per-group-with-.groupby\\(\\))<br/>\n",
    "&emsp;[Sorting columns](#Sorting-columns)<br/>\n",
    "&emsp;[Plotting pandas data](#Plotting-pandas-data)<br/>\n",
    "&emsp;[Sending pandas dataframes to R and vice versa](#Sending-pandas-dataframes-to-R-and-vice-versa)<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "As customary, let's make sure that we're using the `bioinfo` environment that we created in the `Py201` notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see `bioinfo` being printed out.\n",
    "\n",
    "If you're on Windows, remember that every line starting with a `!`, such as `!my code` should be changed to `!wsl my code`, and you need to have an active [WSL installation](https://docs.microsoft.com/en-us/windows/wsl/install-win10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries\n",
    "We will need:\n",
    "\n",
    "Module        | Use\n",
    ":-------------|:-----------------------------------------\n",
    "**pandas**            | Python's dataframe tool.\n",
    "**numpy**             | The Python computation optimiser.\n",
    "**matplotlib.pyplot** | Python's main (but not only!) plotting library.\n",
    "**%matplotlib inline**| This tells Jupyter to throw plots directly onto the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're giving `pyplot` an **alias** of `plt`, `pandas` to `pd`, and `numpy` to `np`. This is completely unnecessary but everybody does it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a `pandas.DataFrame`\n",
    "The easiest way of creating a `pandas.Dataframe` from scratch is with a dictionary, where the keys are the column names and each value is a list of row values for that column.\n",
    "\n",
    "In the example below we define two columns:\n",
    "1. `'threes'`, contains a few multiples of `3`.\n",
    "2. `'squares'` contains the squares of those numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `pandas` adds an extra indexing column which, in traditional Python fashion, starts at `0` and keeps going up one by one, just like the indices in a `list` or in a `numpy.array`.\n",
    "\n",
    "You may remember two things from previous lessons:\n",
    "1. If the last cell is the name of a variable or a simple value, Jupyter prints it out to screen.\n",
    "2. I originally recommended that you _do not_ generally make use this behaviour and instead use `print()` to explicitly print anything that you want printed.\n",
    "I will backtrack from that latter recommendation in the case of `pandas`, because its dataframes are printed much more nicely if you just write their name in the last line on their own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ö†Ô∏è There are many more ways of creating a pandas dataframe!\n",
    "There must be at least a dozen ways to create a pandas dataframe. Above we used the dictionary `column:[values]` method, and below we will read in data from a file. In my view these two are by far the most common and intuitive, but there are many other ways, described in the pandas documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the size(s) of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `shape`, we can trivially extract the number of rows (`df.shape[0]`) and the number of columns (`df.shape[1]`). Another very common and extremely efficient way of getting the number of columns is to determine the length of the `index` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that, let's look a little closer into how to access columns in a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing columns in a dataframe\n",
    "We can extract columns in a number of ways. Two main ones are using:\n",
    "+ **A)_Attribute_ (or _object_) notation:** `df.mycolumn`.\n",
    "+ **B)_Dictionary_ (or _key_) notation:** `df.['mycolumn']`.\n",
    "\n",
    "We used **attribute notation** above to get the `index` column, which is a default column that all pandas dataframes have. But we can also access columns with names that we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and **B)_dictionary_ notation** (with the key in brackets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of object notation is that it looks really neat and is therefore easy to read and quick to write. Conversely, dictionary notation is clunky to write, but it has the advantages that the key can have spaces in its name, and also you can pass the column name (key) as a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, dictionary notation is more universal and _robust_, i.e. it works in most cases, whereas attribute notation does not (e.g. for creating or deleting columns, you must use dict notation).\n",
    "\n",
    "By the way, you may notice that the last line tells us the name of the column (`threes`) but it also tells us that its `dtype` is `int64`, an integer of 64 bits. You may remember this from the `numpy` lesson. Indeed, wherever possible, **`pandas` leverages `numpy` to optimise the contents of dataframes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing multiple columns\n",
    "For this, a good way is to use double brackets, with the desired columns separated by commas inside the inner brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, those were our only two columns, so we just get the entire dataframe again, but you can choose whichever columns you wish, in any order. For example, try putting `'squares'` first and `'threes'` second in the last cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic operations across entire columns\n",
    "We can do basic operations on each of the elements of a column very efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also do operations side-by-side between columns. For example, we can multiply the column with the multiples of `3` by the column that has their squares, to get the cubes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataframe remains unaltered though. So let's see how to keep the result in a new column if we wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding columns to a dataframe\n",
    "For this, you just specify the name of the new column and tell pandas what you want it to contain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Note that you can't use `dot.notation` for the name of the new column, you must use `{'dictionary' : 'notation'}`. However, for the values in the calculation, you can use dot notation if you wish (as we did above for the first calculation) or dict notation (as we did for the second calculation)df['names'] = ['zero', 'three', 'six', 'nine', 'twelve', 'fifteen']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns don't have to be calculated from other columns, you can just add columns with whatever content you wish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting or \"dropping\" columns\n",
    "This is trivially done by two methods. One is using Python's `del` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like for adding columns, you must use dictionary notation to remove a column with `del`, i.e., if you instead had tried:\n",
    "```python\n",
    "del df.squares\n",
    "```\n",
    "you would have gotten an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other method to remove a column is to \"drop\" it by using the pandas method `.drop(columns=[col1, col2, ...])`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è We need to specify `inplace=True` so that we make the change into the dataframe itself, as opposed to returning a changed version of it (the default behaviour)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming columns\n",
    "To rename column(s), pandas uses a dictionary syntax, in which the key is given as `old_name` and the value is the `new_name`. For example, to rename both the `sqroots` and `names` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Just like for dropping a column, we need `inplace=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying functions on columns\n",
    "You can use the `.apply()` method to apply any function you wish to every element in a column. This can be a pre-existing python function, or your own function if you wish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, if you're just going to double a value it's much easier to just multiply it by two, but you get the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fancier still, you can use Python's `lambda` functions to define a novel function on the fly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full understanding of the above is beyond the scope of this tutorial, but I encourage you to look into lambda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data types of columns\n",
    "Easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the strings are called \"`object`\"  in pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the data type of a column\n",
    "One way to change the data type of a column is to reassign it to itself with a different type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of `doublenegs` has changed from `int64` to `float32`. And if we now print the dataframe again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ... we see that the `doublenegs` column has indeed changed from integer to float (it now has a decimal; it's zero in all cases, but it's there)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean operations and filtering/subsetting the _rows_ in a dataframe\n",
    "Row filtering requires a bit of Boolean wizardry: you need to tell pandas which conditions you want your data to match. For example, to find out which of the multiples of three are also even, we ask pandas to show us which of those numbers give a residue (`%`) of `0` when divided by `2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as you can see, the result is a `bool`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering a dataframe with `df[ filter ]`\n",
    "Now that we have a Boolean filter, we can use it to ask pandas to show us _only_ the elements that match that filter. In our example, to get the rows for which the `threes` column is even, we use the same filter as above and feed it to a `[ ]` pair for subsetting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(the spaces inside the brackets are unnecessary, I just put them there for clarity)\n",
    "\n",
    "This may seem a little confusing at first, particularly because of the double `df` outside and inside the brackets. What's happening here is the inner code creates the same list of `True`s and `Falses` that we got above; we then send this list to `df`, so that pandas can decide what to show us from the dataframe: it shows us those rows for which it gets a `True`, skips those that have a `False`.\n",
    "\n",
    "In fact, you could use any combination of six `True`/`False` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying multiple conditions with `( ) | ( )` and `( ) & ( )`\n",
    "Let's get all rows for which either the `threes` column is even, or the value in the `num_name` column contains the text `\"een\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case you can't remember from the basic Python workshop how list comprehensions work, here is one to refresh your memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `.loc()` and `.iloc()` to filter and explore rows and columns\n",
    "There are several ways to access rows. Above we used simply:\n",
    "```python\n",
    "df[ df.threes %2 == 0 ]\n",
    "```\n",
    "But this has limitations, and there are other ways worth exploring. One of the most popular and powerful is `.loc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `.loc` to access elements in the dataframe\n",
    "If you change the first `df` in the code above to `df.loc`, we get what seems to be the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder why you would ever want to write `df.loc` when `df` alone suffices. **The main advantage of `.loc` over simple bracket notation is that `.loc`  gives you access to the dataframe itself**, so you can make changes to it (as we will see soon), whereas **with df[ ] notation you only get a _copy_ of the data, so you can't make any permanent changes to it**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering rows _and_ columns\n",
    "`.loc` can filter columns too. All we need is a way of deciding which columns we want. This could simply be by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you could use _separate_ filters for the row values and column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used a **list comprehension** to tell pandas to get us any column that has the text `\"leneg\"` in its name. You can use any valid filter you can come up with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, what needs to happen here is we must give either specific names of the columns, or a list of `True`s and `False`s for both the rows and the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Note that the rows list must be `6` items long because we have six rows, whereas the columns list must be `5` items long because that's the number of columns in this dataframe at present. Any other lengths will throw an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using .iloc (integer location) to access rows and column by position\n",
    "The `.iloc` property gives us access to the rows of the dataframe by numbered position, starting at 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can also give a range, which works as customarily in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also access the columns in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take another look at the raw dataframe to make sure that it worked well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `.loc` (or `.iloc`) to replace values in a dataframe\n",
    "Let's create a new column called `is_even`, and define it to `False` for starters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where `.loc` shows its powers. If we were to try to use `df[ condition ][column]` to try to change the value, the operation would fail, because `df[ ]` only produces a copy of the data, it doesn't give access to the dataframe itself. In contrast, `.loc` both returns what we're looking for, and it gives us access to it, so we can use it to change values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `.iloc` functions in a similar way to `.loc`, i.e. you can use it to change values as well as just look at them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing and reading data to/from text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to a `.csv` or `.tsv` file\n",
    "This is trivially done. The default is comma-separated values (`CSV`), but if you want tab-separated (`TSV`) instead ‚Äî or something else ‚Äî you can just specify a separator with `sep`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in a `.csv` or `.tsv` file\n",
    "Reading data in is just as easy as writing. We do it with:\n",
    "```python\n",
    "df.read_csv('the_file.tsv', sep='\\t')\n",
    "```\n",
    "You don't need to specify a separator if the data is in `CSV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in a more realistic dataset\n",
    "Since we are already familiar with many of the basics of pandas, let's load in a richer dataset (which you have likely downloaded along with this notebook). This dataset contains a few details on phone and internet connectivity per country that I downloaded from the WorldBank. I did a bit of pre-cleaning in Excel and now this is what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that **this data is in \"stacked\" or \"record\" format**, as is most of the data from the UN, WHO, and WorldBank. We need to transform it so that each country appears only once, and we have the indicator values for each year as columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `pivot` to transform a stacked dataframe to a multi-indexed one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very good. We have a multi-indexed dataframe now. We can now get only the values for 2015:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And if we want to get a column therein, all we have to do is filter twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "And further down the rabbit hole, if we want the values for a specific country, we filter thrice (using the country code, which is our main index):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening a multi-index dataframe\n",
    "The multi-index dataframe that we just built is very cool, but it may get a little annoying for some purposes. For example, you can't immediately access the country name as you normally would by issuing:\n",
    "```python\n",
    "countries = df['Country Name']\n",
    "```\n",
    "...it throws an error. So, let's flatten that dataframe to make it more like a normal pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the columns of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would seem the indices are tuples. We need to flatten those. But we do want to keep the year, so let's do that through a `for` loop that combines both the name of the indicator and the year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we can do some fancy rearranging and replacement of spaces for underscores `_` in the indicator name, and we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now we can use this to feed a list of new column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this list to replace the column names in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking good, but you'll notice that we still have a multi-level index. There are many ways to resolve this, but one I would like here is to add a new index to the dataframe, so that it has consecutive numbers as index like a default pandas dataframe. This will have the added advantage of allowing us to now use the `Country Code` and `Country Name` as regular pandas columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting the index of a dataframe\n",
    "Resetting the index of a dataframe adds a new 0..1..2..N index to the dataframe, and releases any oreset_index column(s) that may have been previously used as index, turning them into regular columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that now our indices are flat.\n",
    "\n",
    "Even better, we can now access our `Country Code` and `Country Name` as regular columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or as multiple columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting more than a sample of rows\n",
    "You will have noticed that `pandas` is very friendly in not printing the entire rows to screen. But what if you do want them all? You can either print the desired column as a list with `list(df.mycol)`, or, to stay within pandas, you can ask for the `.values` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ö†Ô∏è Extracting the `.values` is also helpful for columns with long text, which pandas cuts out by default.‚ö†Ô∏è**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics\n",
    "### General high-level description of the data using `.describe()`\n",
    "We can get a very good summary of the data in the dataframe using the `.describe()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific summary statistics (mean, std, median, min, max, count)\n",
    "The `.describe()` method provides a general view of several useful summary statistics, such as the `mean`, `count`, `median` (i.e., the 50% quantile), `min`, `max`, and standard deviation (`std`). All of these have their own methods too, which you can apply either on the entire dataframe or on specific columns.\n",
    "\n",
    "If you run one of these summary methods on the entire dataframe, pandas calculates it for all possible columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summar statistics on the entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "More typically, you will want to calculate a desired summary statistic on a single column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the positions of minimum and maximum values in a dataframe\n",
    "The `.idxmax()` and `.idxmin()` methods give the index of the rows that contain the maximum and minimum values for a given column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On their own, the indexes of the rows are not very informative, but we can use them to extract the names of the corresponding countries, using `.iloc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's load another very interesting set of country indicator data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Two things worth noting right from the start:\n",
    "+ The name of the file suggests these are values for 2016 only.\n",
    "+ There is no country-code name, which may lead to problems down the line if the names of the countries are not written exactly identically when we try to merge the dataframes.\n",
    "\n",
    "We'll bump into both of these problems below as we attempt to merge the dataframes.\n",
    "\n",
    "For now, let's look at a high-level summary/description of the contents of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very useful. For example, the **count** row tells us that only `36` countries have an entry for the `% Literacy`, so these two columns may not be particularly useful, and we may do better to just drop them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging dataframes\n",
    "We now want to build a Frankensteined dataframe with both sets of data (the one in `df` and the one in `df2`). Pandas provides two very handy functions for this purpose: `join` and `merge`. With `merge`, we can specify which column we want to use to merge the two dataframes. Unfortunately, `df2` doesn't have a `Country Code` column, so we will have to use the more dangerous `Country Name`. This is problematic. See what happens if we just try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, the columns are now all together, but take a look at the number of rows (which we do by getting the length of the `index` column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears we've lost `5` countries! Since we used the country names to merge, that must have been where the problem was. Let's compare the two columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing two columns\n",
    "There are many ways to compare columns (e.g. you could use a `for` loop and `if` statements in regular python), but here's a fancy pandas way to find the countries in `df2` that are not (`~`) in the column `Country Name` of the first `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I do not expect you to fully understand what that code is doing. For now, just do what most people do: copy and paste it from StackOverflow and then try to understand it if you want to and have the time, otherwise just make sure that it does what you need and move on)\n",
    "\n",
    "And let's do it the other way around, to find the countries that are in the first but not the second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the same, but the differences are extremely subtle in terms of spelling, except for Eswatini, which whoever created the second dataframe failed to get the memo that the country is no longer called Swaziland.\n",
    "\n",
    "The most interesting problem is with the DPRK, which has a `‚Äô` in one of the dataframes and a `'` in the other... spot the difference? Many thousands of programmer hours are wasted every day because of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing specific values in a dataframe with `.replace()`\n",
    "Above we used a Boolean filter to change all the values in the dataframe that matched the filter. However, here I cannot think of any clever programmatic way of solving the problem of the five differing names; we will simply have to change each of them one by one. Pandas provides the method `.replace(old_value, new_value, inplace=False)` for this.\n",
    "\n",
    "It doesn't matter which of the two dataframes we do it in, or we could do it in both, as long as we end up with the same name in both (e.g. we could make both dataframes `eSwatini`, which is how I believe they like to spell it there, but we'll keep it simple here).\n",
    "\n",
    "Let's change the five differing values in the second dataframe only. But since we're clever, we won't do it entirely by hand each time; we'll build a dictionary and do a `for` loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `inplace=True` so that we make the change into the dataframe itself, as opposed to returning a changed version.\n",
    "\n",
    "Let's look for differences again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the result is empty (there are no items that are in `df2` but not in `df1`), so we can go ahead and create the merged dataframe again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice. We could now save it to a file so that we can use it later if we wished by running:\n",
    "```python\n",
    "wbmerged.to_csv('WB_merged.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing aggregate statistics per group with `.groupby()`\n",
    "Now that we have a masterfully curated dataset, we can do some actualy data science on it. For example, let's compare the `CO2 emissions (metric tons per capita)` by income group of the country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Gosh... if there's a hell, those of us living in high-income nations are probably headed straight there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we don't really have to specify a column at the end, we can just get the mean for all values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides `mean()`, other functions that we may want to use are `count()`, `sum()`, `min()`, `max()`, `std()` and `median()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting columns\n",
    "Let's take those latter result and sort them by the most sinful carbon producers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...well, no surprises there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about that 2016 problem?\n",
    "Our second imported data has only the values for 2016, whereas the first one had 2015 to 2019. If we want to analyse them together, should we keep the 2019 data from the first one since it's more recent, or should we use the 2016 data only so that it matches the status at the time of the other data? Both approaches are defensible, and in your research you'll need to make decisions like this. Just make sure to explain it clearly to yourself and your colleagues/reviewers/peers/readers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting `pandas` data\n",
    "Pandas makes plotting its data extremely straightforward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting a `pandas` dataframe is extremely easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the default plotting software to Plotly\n",
    "By default, `pandas` uses Matplotlib as its default plotting software. This can be changed to the more interactive Plotly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use Plotly just like we used Matplotlib above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that **Plotly graphs are dynamic!** Scroll over the values to see further details in pop-up clouds. You can also select a box to zoom into, and double-click to get back to full view. Plotly is extremely powerful, and I strongly recommend it if you are considering advanced data visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending `pandas` dataframes to R and vice versa\n",
    "You could of course export to TSV or CSV and then import into R in R-Studio, or vice versa, and sometimes that's the best solution. But you can in fact send dataframes between the two languages right here within Jupyter. We will cover that in the **_Interfacing R with Python_** lesson, but here's a quick demonstration of how easy this is.\n",
    "\n",
    "First, we load the R extension for Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can declare a full cell as R code, import the dataframe into R, and from then on we just do our normal everyday R, including plotting and such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è This was only a brief demonstration; please see the **_Interfacing R with Python_** notebook for a more thorough introduction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
