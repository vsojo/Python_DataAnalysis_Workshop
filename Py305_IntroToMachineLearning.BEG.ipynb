{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beneficial-preservation",
   "metadata": {},
   "source": [
    "# A very timid introduction to machine learning with scikit-learn\n",
    "(Víctor Sojo | vsojo@amnh.org)\n",
    "\n",
    "Here we will provide an all-too-brief introduction to some of the main concepts of machine learning with scikit-learn (and pandas). We will look at only one type of model, linear regression, to demonstrate the key principles of working with scikit-learn.\n",
    "\n",
    "**References:**\n",
    "+ The [scikit-learn tutorial](https://scikit-learn.org/stable/tutorial/basic/tutorial.html#conventions).\n",
    "+ The [Wikipedia article on Machine Learning](https://en.wikipedia.org/wiki/Machine_learning).\n",
    "+ The data comes from the amazing [World Bank Open Data repository](https://data.worldbank.org/indicator)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-factory",
   "metadata": {},
   "source": [
    "## Contents\n",
    "&emsp;[What is machine learning?](#What-is-machine-learning?)<br/>\n",
    "&emsp;[Getting started](#Getting-started)<br/>\n",
    "&emsp;&emsp;[Checking that we're running the right environment](#Checking-that-we're-running-the-right-environment)<br/>\n",
    "&emsp;&emsp;[Installing scikit-learn \\(if you don't have it\\)](#Installing-scikit-learn-\\(if-you-don't-have-it\\))<br/>\n",
    "&emsp;&emsp;[Importing necessary libraries](#Importing-necessary-libraries)<br/>\n",
    "&emsp;[Linear Regression](#Linear-Regression)<br/>\n",
    "&emsp;&emsp;[Creating a filtered dataframe](#Creating-a-filtered-dataframe)<br/>\n",
    "&emsp;[Removing missing values from the pandas dataframe](#Removing-missing-values-from-the-pandas-dataframe)<br/>\n",
    "&emsp;&emsp;[Visualising the columns](#Visualising-the-columns)<br/>\n",
    "&emsp;&emsp;[Building the linear regression model](#Building-the-linear-regression-model)<br/>\n",
    "&emsp;[Predicting values using the model fit](#Predicting-values-using-the-model-fit)<br/>\n",
    "&emsp;[A bit of machine-learning philosophy](#A-bit-of-machine-learning-philosophy)<br/>\n",
    "&emsp;[Using train-test-split to test a model](#Using-train-test-split-to-test-a-model)<br/>\n",
    "&emsp;&emsp;[Split the dataset into ~75% training and ~25% testing](#Split-the-dataset-into-~75%-training-and-~25%-testing)<br/>\n",
    "&emsp;&emsp;[Train the model with the training set](#Train-the-model-with-the-training-set)<br/>\n",
    "&emsp;&emsp;[Judging the adequacy of the model using the testing set](#Judging-the-adequacy-of-the-model-using-the-testing-set)<br/>\n",
    "&emsp;[A more complex model with multiple predictors](#A-more-complex-model-with-multiple-predictors)<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-packing",
   "metadata": {},
   "source": [
    "## What is machine learning?\n",
    "Most educated people younger than 60 nowadays tend to have some general idea of what machine learning is, or at least what it does. In a nutshell:\n",
    "Given some input data, the computer makes a prediction about that data (i.e., a prediction that wasn't necessarily immediately obvious to a human by simply looking at the data).\n",
    "\n",
    "There are two main types of machine learning:\n",
    "+ **Supervised learning** takes a (preferrably large) set of inputs whose outcomes are known as \"training data\", and then gives predictions on novel \"test data\" based on how much it ressembles the training data. Supervised learning is itself divided into two main sub-categories:\n",
    "  + **Regression:** We want to predict one or more outcome values given one or more input values (e.g., using a slope and intercept to get the value of $y$ given $x$).\n",
    "  + **Classification**: Given some input data, we want to determine which of a number of possible types (classes) it belongs to.\n",
    "+ **Unsupervised learning** looks for patterns within the data itself, (in principle) without comparing it to anything else. This could be for example grouping some elements of the data with similar ones into clusters.\n",
    "\n",
    "In this notebook we will use linear regression to demonstrate the key principles of machine learning and working with `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-alloy",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "### Checking that we're running the right environment\n",
    "As customary, let's confirm that we're in the `data` environment, which we created in the Py301 notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-grass",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "alike-anatomy",
   "metadata": {},
   "source": [
    "### Installing scikit-learn (if you don't have it)\n",
    "Just in case you haven't installed scikit-learn yet, do the following, remembering to change the `!` to `!wsl` if you're on Windows, or remove it altogether if you're doing it on the terminal(Mac|Linux)/Anaconda-Prompt(Win):\n",
    "```python\n",
    "!conda install -y scikit-learn\n",
    "```\n",
    "I already have it, so I commented the following cell out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-uniform",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "creative-realtor",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-cooperative",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-ensemble",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "Perhaps you're already rather familiar with the most straightforward (pun intended) type of machine learning: linear regression., you have a straight line and you can use the slope and intercept to predict a new value of Y given X (or vice versa).\n",
    "\n",
    "You must have download a list of country indicators that I built up from World Bank data. We can read this into `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-reward",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "verbal-florist",
   "metadata": {},
   "source": [
    "It seems we have a lot of missing values (`NaN`). That's not good for scikit-learn, which needs existing data in all cells. We will therefore have to create clean dataframes for each of our analyses.\n",
    "\n",
    "We can show the number of rows with missing data for each column in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-yemen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "terminal-probe",
   "metadata": {},
   "source": [
    "### Creating a filtered dataframe\n",
    "We will create a new dataframe called `dfr` containing only the desired columns for this regression (plus country names, just in case). We do this because we'll need to drop the `NaN`s before running the model in `scikit-learn`, and we don't want to alter the original dataframe.\n",
    "\n",
    "Let's say we want to build a model that allows us to predict the **life expectancy** of a country by **how many mobile-phone subscriptions** there are in that country. This may seem a little ridiculous, and to a point it is – although a case can certainly be made that more technology correlates with wealth which correlates with health which correlates with life expectancy. Let's take a look anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-thesis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "corporate-dividend",
   "metadata": {},
   "source": [
    "## Removing missing values from the pandas dataframe\n",
    "We still have a lot of `NaN`s, so let's clear them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-affiliate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "naughty-learning",
   "metadata": {},
   "source": [
    "You'll see that `American Samoa` and `Andorra`, which didn't have a value for cellphone subscriptions, have been dropped from the dataframe, as have all the other rows that didn't have a value in our chosen columns. That's why our new dataframe has only `201` columns, as opposed to `217` before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-berkeley",
   "metadata": {},
   "source": [
    "### Visualising the columns\n",
    "Before we go any farther, let's take a look at what we're regressing – **It's always a very good idea to plot your data before you run any models on it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-windsor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ideal-engineer",
   "metadata": {},
   "source": [
    "...well... I'm not too sure how great a linear relationship this will be, but let's see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-despite",
   "metadata": {},
   "source": [
    "### Building the linear regression model\n",
    "Now we can build a linear regression model using scikit-learn's `LinearRegression` module, which we imported at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-missouri",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "military-matrix",
   "metadata": {},
   "source": [
    "We must extract our data in the appropriate shape, which must be an $n\\times1$ array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-teaching",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "vocational-hostel",
   "metadata": {},
   "source": [
    "And now we can fit our model using the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-sheep",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "varied-romance",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-statement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "surgical-bracket",
   "metadata": {},
   "source": [
    "And we can now use this to plot the line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-husband",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-romantic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efficient-lithuania",
   "metadata": {},
   "source": [
    "A horrendous fit for sure. Let's take a look at the $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-discovery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "middle-productivity",
   "metadata": {},
   "source": [
    "Ow well, That wasn't too good, but there you have it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-climb",
   "metadata": {},
   "source": [
    "## Predicting values using the model fit\n",
    "Let's say you still want to press on with your rather questionable idea that mobile cell subscriptions are a great _linear_ predictor of life expectancy. We could now use this model, which we fit above using available countries, to predict life expectancy in other countries not included in our original list, for example, here is the mobile usage in a few lands not included above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-perspective",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-forum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-teach",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-cutting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "different-fleece",
   "metadata": {},
   "source": [
    "Clearly, our model is not a good fit. **Mordor** may be very high on technology, but it is well known that its citizens don't get to live very long lives.\n",
    "\n",
    "I deliberately chose a terrible model here. There are multiple reasons why this is bad, not only because the predictor is not a great one, but also because life expectancy cannot possibly increase linearly, regardless of what the predictor is.\n",
    "\n",
    "The key take-home message here is: **you need to choose the appropriate model _and_ the appropriate predictor(s) for your data**. This is far from trivial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-flower",
   "metadata": {},
   "source": [
    "## A bit of machine-learning philosophy\n",
    "I don't suppose I need to labour this point, but: **correlation does not imply causation**. Yet, to a point, machine learning just shrugs its shoulders at that statement and says **_Sure, but who cares?!_**. The fact is, for many machine-learning applications, it doesn't really matter _why_ you can predict something as long as you can predict it _reliably_. You can always then go back and try to figure out why [margarine consumption would correlate with divorce rates](https://tylervigen.com/spurious-correlations), or whatever. The answer may be:\n",
    "+ **a)** The two factors correlate simply because of pure luck, and I shouldn't use them as predictors of each other because there's no reason to think they'll still correlate in the future.\n",
    "+ **b)** They truly do correlate because of direct causal reasons; there is a legitimate, driving connection between the two that I haven't discovered yet.\n",
    "+ **c)** They correlate only because something else causes both things to vary similarly, but they themselves are entirely unrelated.\n",
    "\n",
    "The last one is interesting: The fact that two things correlate because of an underlying cause that is unknown doesn't mean the two known factors are any less useful as predictors of each other. If the connection to the true causal factor is solid in both cases, then the two affected values can be reliable predictors of each other, even if we ignore the true cause of the connection. This may not be great hypothesis-based science, but it is darn useful for machine-learning and in scenarios such as health and finance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-sherman",
   "metadata": {},
   "source": [
    "## Using `train-test-split` to test a model\n",
    "In the last example we made up a few countries to test our model. However, in a more realistic scenario, you will want to make sure your model is making adequate predictions before you run it on unknown data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-positive",
   "metadata": {},
   "source": [
    "Let's do a more reasonable prediction here. Let's try seeing how life expectancy correlates with how likely (on average) girls in a country are to complete the lower part of secondary school.\n",
    "\n",
    "We first extract the desired columns and create the filtered dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-genome",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "tribal-worry",
   "metadata": {},
   "source": [
    "As before, let's plot before we do anything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-painting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "gothic-afternoon",
   "metadata": {},
   "source": [
    "This looks better than the mobile-phone plot.\n",
    "\n",
    "As you can see, `matplotlib` just ignores the `NaN`s, but `scikit-learn` breaks if they are there, so let's drop them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-compatibility",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "micro-springer",
   "metadata": {},
   "source": [
    "And now we could run the model with the whole data as above, but if we did that we'd be left without any way to test how accurate it is. Instead, let's split our data into a **training** set (traditionally 75% of the data is chosen to train the model) and a **testing** set (traditionally the remaining 25%).\n",
    "\n",
    "After splitting our data, we will train the model with the training set, and then testing with the testing set (surprise!). We will then be ready to receive novel data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-waters",
   "metadata": {},
   "source": [
    "### Split the dataset into ~75% training and ~25% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-router",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "moderate-married",
   "metadata": {},
   "source": [
    "That's as close to `75%:25%` as you can get with this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-engine",
   "metadata": {},
   "source": [
    "### Train the model with the training set\n",
    "Good, now that we have a training set with `89` out of the `119` samples, we can train our model just like we did above, but using only the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-gateway",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "geological-grace",
   "metadata": {},
   "source": [
    "### Judging the adequacy of the model using the testing set\n",
    "Now that we have a model and know how it `score`s with its own data, let's check how it fares with the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-genetics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-nickel",
   "metadata": {},
   "source": [
    "Great! That looks pretty good. We're ready to now test with _real-World_ data, pun very much intended: we can take any countries that have a `LowSecondary_completion_female` but no `Life_expectancy` and predict the latter using our model (or we could do the opposite too)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-fusion",
   "metadata": {},
   "source": [
    "## A more complex model with multiple predictors\n",
    "So far we have been using simple one-to-one predictions, but there is absolutely no reason we need to limit ourselves in this way.\n",
    "\n",
    "Let's take another high-level view at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-church",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "educational-poison",
   "metadata": {},
   "source": [
    "Let's use the rate of primary-school completion for females, gross national income (GNI), broadband subscriptions, and CO<sub>2</sub> emissions, to predict **life expectancy**.\n",
    "\n",
    "We first define a dataframe with the desired columns, keeping the country for reference, and we drop its empty values (`NaN`s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-accent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "thirty-arthritis",
   "metadata": {},
   "source": [
    "Only 121 countries out of the original 217 have data for all the desired columns. It's always worth noting how much we lose by filtering (maybe we should remove some columns to increase coverage)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-philadelphia",
   "metadata": {},
   "source": [
    "Creating the model itself is rather easy and similar to doing it with a single predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-marshall",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "announced-mouth",
   "metadata": {},
   "source": [
    "Note that we are now using uppercase `X`, as opposed to lowercase `x` above. Uppercase `X` and lowercase `y` is traditional usage in machine learning circles (at least in Python), meant to signal that there are _many_ predictors for _one_ outcome.\n",
    "\n",
    "Other than this, by convention, Python variables should always be named with starting lowercase unless you have very good reason to do otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-dover",
   "metadata": {},
   "source": [
    "We can now take a look at the coefficient (slope):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-storm",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-assist",
   "metadata": {},
   "source": [
    "It turns out we don't have a single coefficient this time, but `4`, since we used that many predictors. We measly humans with our flat brains cannot visualise that, so we won't bother trying to plot it, but we can certainly check how good the fit is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-agriculture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "republican-florist",
   "metadata": {},
   "source": [
    "That's a rather decent r<sup>2</sup>... but do remember that life expectancy should not grow linearly (as far as we know anyway), so linear regression is probably not a great model for it. We measly humans are bound to perish (it seems), so we can't just extend our lives endlessly by simply pumping more CO<sub>2</sub> into the atmosphere, whatever our model may say... not that this will stop us trying to pump CO<sub>2</sub> into the atmosphere, but anyway.\n",
    "\n",
    "However, what exactly is each of the four predictors doing for our model? We may need to take some of them out and see if our ability to predict remains similar (e.g., I suggest you try dropping the CO<sub>2</sub> emissions). We can also do more advanced statistical analyses to gauge the influence of each of our predictors.\n",
    "\n",
    "There is a wide world of possibilities just with this one linear regression, and of course there is far more that can be done with scikit-learn. I hope this notebook has given you a useful introduction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
